{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amnSfQ4S-im9"
   },
   "source": [
    "# Data Version Control and Experiment Tracking with DVC and Dagshub!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will take a fraud detection model we have built with transaction data and learn how to version our dataset and track our experiments. We will use two tools to do this, DVC and DagsHub.\n",
    "\n",
    "### Why should you version your dataset and keep track of experiment results?\n",
    "\n",
    "Why do we need to version our data? Well simply put, we need to make sure that we can reproduce our experiments accurately and reliably. Without a version control system, it makes it cumbersome to guarantee reproducible experiments, and to know which experiments have been successful. This becomes amplified when you are dealing with teams, where you may have multiple people working on the same model producing different results. Specifically, data versioning and experiment tracking aim to solve the following problems:\n",
    "- Maintain our datasets and models in the same manner as our codebase.\n",
    "- Ensure that our experiments are reproducible and reliable.\n",
    "- Enable collaboration with other team members so that we can maintain a single source of truth for our data and experiments.\n",
    "\n",
    "### When to version your data and use experiment tracking?\n",
    "\n",
    "Ideally, as this system is relatively easy to setup and provides utility even as a solo data scientist, it should be implemented in most cases. As a solo data scientist, you can simply version your data and experiment results as you go, without maintaining external spreadsheets of results and manually versioning data you use. In team settings, systems such as these become indespensible tools for seamless collaboration, making sure you can share results and use more traditional software engineering methodologies to maintain your repository such as code review, devops, and CI/CD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tut Prerequisites\n",
    "\n",
    "Prerequisites:\n",
    "- Install Docker\n",
    "- Install Python3.8+\n",
    "- Install JupyterLab\n",
    "- Install Git\n",
    "- Create a Github Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nY8jTgnQ_P08"
   },
   "source": [
    "By the end of this tutorial you will be able to:\n",
    "- Setup DVC for version controlling datasets and models\n",
    "- Link your GitHub repository to DagsHub\n",
    "- Use DagsHub to track your experiments\n",
    "  \n",
    "You should download the data required for this tutorial from [here](https://drive.google.com/file/d/1MidRYkLdAV-i0qytvsflIcKitK4atiAd/view?usp=sharing). This is originally from a [Kaggle dataset](https://www.kaggle.com/competitions/ieee-fraud-detection/data) for Fraud Detection. Place this dataset in a `data` directory in the root of your project. You can run this notebook either in VS Code or Jupyter Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a number of libraries for this tutorial so boot up a terminal and install them before you proceed.\n",
    "\n",
    "```bash\n",
    "pip -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Versioning with DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we version our software projects, we typically use a version control system such as [git](https://git-scm.com). Using a system like this allows us to keep track of any changes we make to the codebase, easily collaborate with others by merging versions, and revert to earlier versions if something breaks. Git does have a limitation; it is not built to handle large files or binary files like those used in model building. DVC is a version control system built for datasets and models. You can think of it like git, but allows you to version both large files and model files, treating them in the same way as git.\n",
    "\n",
    "Since you have forked this repo on github, you already have git version control active. You can now use DVC to version your data and models. Initialise DVC, and commit the changes DVC made to git.\n",
    "    \n",
    "```bash\n",
    "dvc init\n",
    "git commit -m \"Initialise DVC\"\n",
    "```\n",
    "\n",
    "Our git repo is now a DVC repo too!. Let's add the `data` directory we created to DVC.\n",
    "\n",
    "```bash\n",
    "dvc add data\n",
    "git add data.dvc .gitignore\n",
    "git commit -m \"Add data directory to DVC\"\n",
    "```\n",
    "\n",
    "Now push your changes to Github!\n",
    "```\n",
    "git push -u origin master\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how can version your data and models with DVC! We now want a central remote repository that deals with DVC repos. Enter DagsHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DagsHub Setup\n",
    "\n",
    "While it is useful to be able to use DVC to version our datasets and models, we need to have a central remote store that will take advantage of the DVC versioning system in the same manner that Github takes advantage of git. This is what DagsHub will allow us to do, acting not only as a source of truth for our codebase, but also as a central repository for our data, data pipeline, models and experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to [DagsHub](dagshub.com) and sign up for a free account. You can login with your Github account.\n",
    "\n",
    "You should be greeted with the following screen. Click the **Connect** button and select **Github**. \n",
    "\n",
    "![DagsHub Entry](media/dags_entry.png)\n",
    "\n",
    "You will be prompted to connect a Github repository. Search for repo fork in and connect! You'll be greeted with a familiar looking screen. This is your repo page. You should refer back to this page throughout the tutorial.\n",
    "\n",
    "![DagsHub Repo](media/dags_repo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want our data to be viewable in DagsHub, we need to add our dataset to DVC and set the DVC remote to our DagsHub repo.\n",
    "\n",
    "```bash\n",
    "dvc remote add origin --local <https://dagshub.com/><username>/<repo_name>.dvc\n",
    "```\n",
    "\n",
    "Now we need to tell DVC how to auth.\n",
    "\n",
    "```\n",
    "dvc remote modify origin --local auth basic\n",
    "dvc remote modify origin --local user <username>\n",
    "dvc remote modify origin --local ask_password true\n",
    "```\n",
    "\n",
    "Puede fallar, la anternativa es:\n",
    "\n",
    "```\n",
    "dvc remote add origin-s3 s3://dvc\n",
    "dvc remote modify origin-s3 endpointurl <https://dagshub.com/><user-name>/<repo-name>.s3\n",
    "dvc remote modify origin-s3 --local access_key_id <Token>\n",
    "dvc remote modify origin-s3 --local secret_access_key <Token>\n",
    "\n",
    "```\n",
    "\n",
    "Before you push your DVC repo, navigate to your DagsHub settings and create a *password* if you do not have one set. Then push! (It may take a while)\n",
    "```\n",
    "dvc push -r origin-s3\n",
    "```\n",
    "\n",
    "You can now view your data in your DagsHub repo. Our file is a little too big to view in the dashboard, bit you can view the raw file to verify it is working.\n",
    "\n",
    "To show you how this updates, let's add some more files to the data directory. Using DVC, run the `preprocess.py` script to add some more files to generate train-test splits of the data to be used in training and testing. \n",
    "\n",
    "We could run the file as is with vanilla python, but we can also use DVC to specify a *pipeline stage*. This tells DVC what the inputs and outputs are for this particular command, which can be used to determined whether in the future the stage needs to be run again with a future build. With the following command we will create a stage called *preprocessing*, and specify all the inputs and outputs of the stage, as well as the processing file itself. In this case, we specify the name of the stage with `-n`, all inputs with `-d` and all outputs with `-o`, and finally specify the actual command to run for the stage as the last argument.\n",
    "```bash\n",
    "dvc run -n preprocessing -d data/train_transaction.csv \\\n",
    "-d preprocess.py \\\n",
    "-o feature_sets/X_train.csv \\\n",
    "-o feature_sets/X_test.csv \\\n",
    "-o feature_sets/y_train.csv \\\n",
    "-o feature_sets/y_test.csv \\\n",
    "python preprocess.py\n",
    "```\n",
    "\n",
    "This should generate 6 new files, `X_train.csv`, `X_test.csv`, `y_train.csv`, and `y_test.csv`, as well as new DVC files, `dvc.lock` and `dvc.yaml`. These new DVC files specify how the data pipeline works, the data versioning, and determines which pipeline stages need to be rerun upon rebuild. Add them to the git and DVC repo, commit and push.\n",
    "\n",
    "```\n",
    "git add dvc.lock dvc.yaml feature_sets/.gitignore\n",
    "git commit -m \"Add generate preprocessed dataframes\"\n",
    "git push -u origin master\n",
    "dvc push -r origin\n",
    "```\n",
    "\n",
    "You can now have a look at your Data Pipeline in DagsHub! You can keep adding new stages to the pipeline, and DVC will automatically determine which stages need to be run while DagsHub will visualise it. We will create a new stage in the next section to see how this works.\n",
    "\n",
    "![DagsHub Data Pipeline](media/dags_data_pipeline.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DagsHub doesn't only supply you with a remote DVC store, but also allows you to track your experiments along with your versioned code and data. We are going to modify the supplied file, `train.py`, to show how this is done. At the top of the file, add the following:\n",
    "\n",
    "```python\n",
    "...\n",
    "from xgboost import XGBClassifier\n",
    "import dagshub\n",
    "...\n",
    "```\n",
    "\n",
    "Now, we are going to wrap our training code in a `dagshub_logger` context manager. The logger has 2 methods, `log_hyperparams` and `log_metrics`, for logging hyperparameters and metrics respectively (duh). Modify the `XGBClassifier` and `.fit()` lines with one indent and wrap it in a `dagshub.dagshub_logger` object as is below. If you'd like, you can be more specific about which hyperparameters you want to log by specifying them manually to the logger (we just use `.get_params()` here).\n",
    "```python\n",
    "...\n",
    "with dagshub.dagshub_logger() as logger:\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"binary:logistic\",\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27,\n",
    "    )\n",
    "\n",
    "    # Log your hyperparameters with DagsHub\n",
    "    logger.log_hyperparams(model_class=type(xgb).__name__)\n",
    "    logger.log_hyperparams({'model': xgb.get_params()})\n",
    "\n",
    "    model = xgb.fit(X_train, y_train)\n",
    "    y_pred = xgb.predict(X_test)\n",
    "\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), 3)\n",
    "    roc_auc = round(roc_auc_score(y_test, y_pred), 3)\n",
    "    \n",
    "    # Log your metrics with DagsHub\n",
    "    logger.log_metrics(\n",
    "        {'accuracy': accuracy}\n",
    "    )\n",
    "    logger.log_metrics(\n",
    "        {'roc_auc': roc_auc}\n",
    "    )\n",
    "...\n",
    "```\n",
    "\n",
    "Save the changes and git commit. Time to run an experiment! Run your modified training file in a new DVC pipeline stage. You may notice we have a new flag `-M`. This tells DVC this is a *metric* output to be tracked by git. There is another option `-p`, which allows us to specify a hyperparameter file as a pipeline dependency. In this case, we are tracking `params.yml` as a metric artifact instead as we are specifying the hyperparameters in code and this file is output by the DagsHub logger.\n",
    "```bash\n",
    "dvc run -n training -d feature_sets/X_train.csv \\\n",
    "-d feature_sets/X_test.csv \\\n",
    "-d feature_sets/y_train.csv \\\n",
    "-d feature_sets/y_test.csv \\\n",
    "-d train.py \\\n",
    "-o models/xgb-fraud-classifier.joblib \\\n",
    "-M metrics.csv \\\n",
    "-M params.yml \\\n",
    "python train.py\n",
    "```\n",
    "\n",
    "This creates 3 new files, our 2 DagsHub files `metrics.csv` and `params.yml`, and our model file `models/xgb-fraud-classifier.joblib`. Add the Dagshub files to the git repo, the model file to dvc, and commit. It is also a good idea to tag your commit with some sort of model version.\n",
    "```bash\n",
    "git add metrics.csv params.yml models.dvc .gitignore\n",
    "git commit -m \"Train XGBoost model with OHE features, v0.1\"\n",
    "git tag -a \"v0.1\" -m \"xgb model v0.1\"\n",
    "git push -u origin master\n",
    "dvc push -r origin\n",
    "```\n",
    "\n",
    "You will see that now you can view your experiments in DagsHub under the **Experiments** tab.\n",
    "\n",
    "![DagsHub Experiments](media/dags_experiments.png)\n",
    "\n",
    "You will also see your updated data pipeline.\n",
    "\n",
    "![Updated DagsHub Data Pipeline](media/dags_data_pipeline_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a new experiment\n",
    "\n",
    "Say we are testing a new hypothesis and want to run a new experiment with the same model, but with a different set of hyperparameters. After some analysis, we change the *n_estimators* and *max_depth* hyperpameters in the `training` file, finalising them to the following. Save the file modification.\n",
    "\n",
    "```python\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27,\n",
    ")\n",
    "```\n",
    "\n",
    "We can now run the experiment again. Since we have specified how each pipeline should be run previously, we now simply tell dvc to *reproduce* the entire pipeline.\n",
    "```bash\n",
    "dvc repro\n",
    "```\n",
    "\n",
    "Great! Push your changes and view your new experiment in DagsHub.\n",
    "\n",
    "![DagsHub New Experiment](media/dags_experiments_new.png)\n",
    "\n",
    "You are now armed with the tools to successfully run experiments and track your results in DagsHub. You can visit each commit of a experiment seperately to see the exact code and data that was used to train the model in that particular run. Reproducing that result is simply a matter of checking out the commit and running the pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "explore.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dagshub')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07b860c21ef0cbdcbcdef8e8b843e6c3698a6256580ce428119cee16b8298f72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
